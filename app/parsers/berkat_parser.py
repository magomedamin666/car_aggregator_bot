import asyncio
import logging
import os
import re
from datetime import datetime, timezone
from typing import Dict, List, Optional

import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from app.bot.telegram_bot import send_ad_notification
from app.db.crud import get_ad_by_source_external, get_all_active_filters
from app.db.models import Ad, FilterSet, SentNotification
from app.db.session import async_session


os.makedirs("logs", exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/berkat_parser.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


BASE_URL = "https://berkat.ru"
SEARCH_URL = "https://berkat.ru/avto"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Referer": f"{BASE_URL}/",
}


async def fetch(session: aiohttp.ClientSession, url: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL."""
    try:
        async with session.get(url, headers=HEADERS, timeout=15) as resp:
            if resp.status == 200:
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                return await resp.text()
            logger.warning(f"[{resp.status}] {url}")
            return None
    except asyncio.TimeoutError:
        logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
        return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
        return None


def matches_filter(ad: Dict, filter_set: FilterSet) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Ñ–∏–ª—å—Ç—Ä–∞."""
    try:
        filters = filter_set.filters_json
        ad_title = ad.get("title", "")[:40]
        filter_name = filter_set.name

        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞: '{ad_title}...' vs —Ñ–∏–ª—å—Ç—Ä '{filter_name}'")

        ad_parsed_at = ad.get("parsed_at")
        filter_created_at = filter_set.created_at

        if ad_parsed_at and filter_created_at:
            filter_created_naive = (
                filter_created_at.replace(tzinfo=None)
                if filter_created_at.tzinfo
                else filter_created_at
            )
            if ad_parsed_at < filter_created_naive:
                logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—à–µ —Ñ–∏–ª—å—Ç—Ä–∞")
                return False

        brand = filters.get("brand")
        if brand:
            ad_brand = ad.get("brand", "").lower().strip()
            filter_brand = brand.lower().strip()

            BRAND_SYNONYMS = {
                "lada": [
                    "lada",
                    "–ª–∞–¥–∞",
                    "–≤–∞–∑",
                    "–≤–∞–∑–∞",
                    "–≤–∞–∑–∏–∫",
                    "–∂–∏–≥—É–ª–∏",
                    "–∂–∏–≥—É–ª—å",
                    "–∫–ª–∞—Å—Å–∏–∫–∞",
                    "–∫–æ–ø–µ–π–∫–∞",
                    "—à–µ—Å—Ç–µ—Ä–∫–∞",
                    "—Å–µ–º–µ—Ä–∫–∞",
                    "–≤–æ—Å—å–º–µ—Ä–∫–∞",
                    "–¥–µ–≤—è—Ç–∫–∞",
                    "–¥–µ—Å—è—Ç–∫–∞",
                ],
                "renault": ["renault", "—Ä–µ–Ω–æ", "—Ä–µ–Ω–æ–ª—å", "—Ä–µ–Ω—É–æ", "—Ä–µ–Ω–∞—É–ª—Ç"],
                "kia": ["kia", "–∫–∏–∞", "–∫—å—è", "–∫–∏–∞—Å", "–∫–∏–∞—à–∫–∞"],
                "hyundai": ["hyundai", "—Ö–µ–Ω–¥–∞–π", "—Ö—é–Ω–¥–∞–π", "—Ö–µ–Ω–¥—ç"],
                "nissan": ["nissan", "–Ω–∏—Å—Å–∞–Ω", "–Ω–∏—Å–∞–Ω"],
                "toyota": ["toyota", "—Ç–æ–π–æ—Ç–∞", "—Ç–æ–µ—Ç–∞"],
                "mazda": ["mazda", "–º–∞–∑–¥–∞", "–º–∞–∑–¥—ã"],
                "volkswagen": ["volkswagen", "—Ñ–æ–ª—å–∫—Å–≤–∞–≥–µ–Ω", "–≤–∞–≥–µ–Ω", "–∂—É–∫"],
                "skoda": ["skoda", "—à–∫–æ–¥–∞", "—à–∫–æ–¥–æ–≤—Å–∫–∏–π"],
                "ford": ["ford", "—Ñ–æ—Ä–¥", "—Ñ–æ—Ä–¥—ã"],
                "chevrolet": ["chevrolet", "—à–µ–≤—Ä–æ–ª–µ", "—à–µ–≤—Ä–æ–ª—å"],
                "bmw": ["bmw", "–±–º–≤", "–±—ç—Ö–∞", "–±–µ—Ö–∞", "–±–µ—Ö—É"],
                "mercedes": ["mercedes", "–º–µ—Ä—Å–µ–¥–µ—Å", "–º–µ—Ä—Å"],
                "audi": ["audi", "–∞—É–¥–∏", "–∞—É–¥–∏–∫"],
                "volvo": ["volvo", "–≤–æ–ª—å–≤–æ", "–≤–æ–ª–≤–æ"],
                "subaru": ["subaru", "—Å—É–±–∞—Ä—É", "—Å—É–±–∞—Ä—É—Å"],
                "honda": ["honda", "—Ö–æ–Ω–¥–∞", "—Ö–æ–Ω–¥—É–ª—è"],
                "suzuki": ["suzuki", "—Å—É–∑—É–∫–∏", "—Å—É–∑—É–∫–∏—Å"],
                "mitsubishi": ["mitsubishi", "–º–∏—Ü—É–±–∏—Å–∏", "–º–∏—Ü—É–±–∏—à–∞"],
                "opel": ["opel", "–æ–ø–µ–ª—å", "–æ–ø–µ–ª–µ–∫"],
                "daewoo": ["daewoo", "–¥—ç—É", "–¥–∞–µ–≤—É"],
                "gaz": ["gaz", "–≥–∞–∑", "–≥–∞–∑–µ–ª—å", "–≥–∞–∑–∏–∫"],
                "uaz": ["uaz", "—É–∞–∑", "—É–∞–∑–∏–∫", "–±—É—Ö–∞–Ω–∫–∞"],
                "moskvich": ["moskvich", "–º–æ—Å–∫–≤–∏—á", "–º–æ—Å–∫–≤–∏—á–∏"],
            }

            ad_brand_clean = "".join(c for c in ad_brand if c.isalnum() or c in " -_")
            matched_brand = False

            for canonical, synonyms in BRAND_SYNONYMS.items():
                if filter_brand in synonyms or filter_brand == canonical:
                    for syn in synonyms:
                        if syn in ad_brand_clean or ad_brand_clean in syn:
                            matched_brand = True
                            logger.info(f"–ë—Ä–µ–Ω–¥: '{filter_brand}' –Ω–∞–π–¥–µ–Ω –∫–∞–∫ '{syn}'")
                            break
                    if matched_brand:
                        break

            if not matched_brand:
                logger.info(f"–ë—Ä–µ–Ω–¥: '{filter_brand}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ '{ad_brand_clean}'")
                return False

        model = filters.get("model")
        if model:
            ad_model = ad.get("model", "").lower().strip()
            filter_model = model.lower().strip()
            if filter_model not in ad_model:
                logger.info(f"–ú–æ–¥–µ–ª—å: '{filter_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ '{ad_model}'")
                return False
            logger.info(f"–ú–æ–¥–µ–ª—å: '{filter_model}' –Ω–∞–π–¥–µ–Ω–∞")

        ad_year = ad.get("year")
        if ad_year is not None:
            min_year = filters.get("min_year")
            max_year = filters.get("max_year")
            if min_year and ad_year < min_year:
                logger.info(f"–ì–æ–¥: {ad_year} < {min_year}")
                return False
            if max_year and ad_year > max_year:
                logger.info(f"–ì–æ–¥: {ad_year} > {max_year}")
                return False
            logger.info(f"–ì–æ–¥: {ad_year} –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ")

        ad_price = ad.get("price")
        if ad_price is not None:
            min_price = filters.get("min_price")
            max_price = filters.get("max_price")
            if min_price and ad_price < min_price:
                logger.info(f"–¶–µ–Ω–∞: {ad_price:,} ‚ÇΩ < {min_price:,} ‚ÇΩ")
                return False
            if max_price and ad_price > max_price:
                logger.info(f"–¶–µ–Ω–∞: {ad_price:,} ‚ÇΩ > {max_price:,} ‚ÇΩ")
                return False
            logger.info(f"–¶–µ–Ω–∞: {ad_price:,} ‚ÇΩ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ")

        ad_mileage = ad.get("mileage")
        if ad_mileage is not None:
            max_mileage = filters.get("max_mileage")
            if max_mileage and ad_mileage > max_mileage:
                logger.info(f"–ü—Ä–æ–±–µ–≥: {ad_mileage:,} –∫–º > {max_mileage:,} –∫–º")
                return False
            logger.info(f"–ü—Ä–æ–±–µ–≥: {ad_mileage:,} –∫–º –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö")

        region = filters.get("region")
        if region:
            ad_region = ad.get("region", "").lower().strip()
            filter_region = region.lower().strip()

            CITY_TO_REGION = {
                "–Ω–∞–∑—Ä–∞–Ω—å": "–∏–Ω–≥—É—à–µ—Ç–∏—è",
                "–º–∞–≥–∞—Å": "–∏–Ω–≥—É—à–µ—Ç–∏—è",
                "–∫–∞—Ä–∞–±—É–ª–∞–∫": "–∏–Ω–≥—É—à–µ—Ç–∏—è",
                "–≥—Ä–æ–∑–Ω—ã–π": "—á–µ—á–Ω—è",
                "—à–∞–ª–∏": "—á–µ—á–Ω—è",
                "–º–∞—Ö–∞—á–∫–∞–ª–∞": "–¥–∞–≥–µ—Å—Ç–∞–Ω",
                "–¥–µ—Ä–±–µ–Ω—Ç": "–¥–∞–≥–µ—Å—Ç–∞–Ω",
                "–º–æ—Å–∫–≤–∞": "–º–æ—Å–∫–≤–∞",
                "–º—Å–∫": "–º–æ—Å–∫–≤–∞",
                "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥",
                "—Å–ø–±": "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥",
                "–ø–∏—Ç–µ—Ä": "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥",
            }

            ad_region_norm = CITY_TO_REGION.get(ad_region, ad_region)
            if filter_region not in ad_region_norm and ad_region_norm not in filter_region:
                logger.info(f"–†–µ–≥–∏–æ–Ω: '{filter_region}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ '{ad_region}'")
                return False
            logger.info(f"–†–µ–≥–∏–æ–Ω: '{filter_region}' –Ω–∞–π–¥–µ–Ω")

        logger.info(f"–£–°–ü–ï–•: –û–±—ä—è–≤–ª–µ–Ω–∏–µ '{ad_title}...' –ü–û–î–•–û–î–ò–¢ –ø–æ–¥ —Ñ–∏–ª—å—Ç—Ä '{filter_name}'")
        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–∞—Ç—á–∏–Ω–≥–µ: {e}", exc_info=True)
        return False


def parse_ad_block(block) -> Optional[Dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ –∏–∑ HTML-–±–ª–æ–∫–∞."""
    try:
        link_tag = block.find("h3", class_="board_list_item_title")
        if link_tag:
            link_tag = link_tag.find("a", href=True)

        if not link_tag or not link_tag.get("href"):
            link_tag = block.find("a", href=lambda href: href and "/content/" in href)

        if not link_tag or not link_tag.get("href"):
            link_tag = block.find("a", href=True)

        if not link_tag or not link_tag.get("href"):
            logger.debug("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –≤ –±–ª–æ–∫–µ")
            return None

        href = link_tag["href"].strip()
        url = urljoin(BASE_URL, href)

        external_id = url.rstrip("/").split("/")[-1].strip()
        if not external_id.isdigit() or len(external_id) < 4:
            external_id = str(hash(url))[:20]

        title_tag = block.find("h3", class_="board_list_item_title") or block.find("a")
        title = title_tag.get_text(strip=True) if title_tag else ""

        title_lower = title.lower()
        non_car_keywords = [
            "—ç–≤–∞–∫—É–∞—Ç–æ—Ä",
            "—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–±–æ",
            "—Ä–µ–º–æ–Ω—Ç",
            "–ø–æ–∫—Ä–∞—Å–∫–∞",
            "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞",
            "—à–∏–Ω–æ–º–æ–Ω—Ç–∞–∂",
            "–∑–∞–ø—á–∞—Å—Ç–∏",
            "–¥–µ—Ç–∞–ª–∏",
            "–∞—Ä–µ–Ω–¥–∞ –∞–≤—Ç–æ",
            "–ø—Ä–æ–∫–∞—Ç –∞–≤—Ç–æ",
            "–≥—Ä—É–∑–æ–≤–æ–π",
            "–≥—Ä—É–∑–æ–≤–∏–∫",
            "–∫–∞–º–∞–∑",
            "–∞–≤—Ç–æ–±—É—Å",
            "–ø—Ä–∏—Ü–µ–ø",
            "–º–æ—Ç–æ—Ü–∏–∫–ª",
            "—Å–∫—É—Ç–µ—Ä",
            "–∫–≤–∞–¥—Ä–æ—Ü–∏–∫–ª",
            "–≤—ã–∫—É–ø –∞–≤—Ç–æ",
            "–∑–∞–ª–æ–≥",
            "–Ω–∞ –∑–∞–ø—á–∞—Å—Ç–∏",
            "–±–∏—Ç—ã–π",
            "–∞–≤–∞—Ä–∏–π–Ω—ã–π",
        ]
        if any(word in title_lower for word in non_car_keywords):
            logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –Ω–µ-–∞–≤—Ç–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {title[:50]}...")
            return None

        known_brands = [
            "lada",
            "–≤–∞–∑",
            "–ª–∞–¥–∞",
            "—Ä–µ–Ω—É–æ",
            "renault",
            "—Ä–µ–Ω–æ",
            "–∫–∏–∞",
            "kia",
            "—Ö–µ–Ω–¥–∞–π",
            "hyundai",
            "—Ç–æ–π–æ—Ç–∞",
            "toyota",
            "–Ω–∏—Å—Å–∞–Ω",
            "nissan",
            "–º–∞–∑–¥–∞",
            "mazda",
            "–º–∏—Ü—É–±–∏—Å–∏",
            "mitsubishi",
            "—à–∫–æ–¥–∞",
            "skoda",
            "—Ñ–æ–ª—å–∫—Å–≤–∞–≥–µ–Ω",
            "–≤–æ–ª–∫—Ü–≤–∞–≥–µ–Ω",
            "volkswagen",
            "vw",
            "–æ–ø–µ–ª—å",
            "opel",
            "—Ñ–æ—Ä–¥",
            "ford",
            "—à–µ–≤—Ä–æ–ª–µ",
            "—à–µ–≤—Ä–æ–ª–µ—Ç",
            "chevrolet",
            "–º–µ—Ä—Å–µ–¥–µ—Å",
            "–º–µ—Ä—Å",
            "–±–º–≤",
            "–±—ç—Ö–∞",
            "–±–µ—Ö–∞",
            "–±–µ—Ö—É",
            "bmw",
            "–∞—É–¥–∏",
            "audi",
            "–≤–æ–ª—å–≤–æ",
            "volvo",
            "—Å—É–±–∞—Ä—É",
            "subaru",
            "—Ö–æ–Ω–¥–∞",
            "—Ö—É–Ω–¥–∞",
            "—Ö–æ–Ω–¥—É",
            "honda",
            "—Å—É–∑—É–∫–∏",
            "suzuki",
            "–¥—ç—É",
            "–¥–∞–µ–≤—É",
            "daewoo",
            "–≥–∞–∑–µ–ª—å",
            "–≥–∞–∑",
            "—É–∞–∑",
            "—É–∞–∑–∏–∫",
            "moskvich",
            "–º–æ—Å–∫–≤–∏—á",
            "—ç–ª–∞–Ω—Ç—Ä–∞",
            "elantra",
            "solaris",
            "—Å–æ–ª–∞—Ä–∏—Å",
            "—Ä–∏–æ",
            "rio",
            "creta",
            "–∫—Ä–µ—Ç–∞",
        ]

        brand = ""
        model = ""
        for brand_candidate in known_brands:
            if brand_candidate in title_lower:
                brand = brand_candidate.capitalize()
                parts = title.split(brand_candidate, 1)
                if len(parts) > 1:
                    model = parts[1].strip()
                break

        if not brand:
            logger.debug(f"–ù–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –º–∞—Ä–∫–∞ –≤: {title[:50]}...")
            return None

        year_match = re.search(r"\b(19[89]\d|20[012]\d)\b", title)
        year = int(year_match.group(1)) if year_match else None

        price = None
        price_tag = block.find(string=re.compile(r"‚ÇΩ|—Ä—É–±|—Ç—ã—Å", re.I))
        if price_tag:
            price_str = price_tag.find_parent().get_text(strip=True)
            price_match = re.search(r"(\d[\d\s]*)", price_str.replace("\xa0", " "))
            if price_match:
                price_text = price_match.group(1).replace(" ", "").replace("\xa0", "")
                try:
                    price = int(price_text)
                    if 10 <= price <= 5000:
                        price *= 1000
                    if price < 5000 or price > 50000000:
                        price = None
                except (ValueError, OverflowError) as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ü–µ–Ω—ã '{price_str}': {e}")
                    price = None

        mileage = None
        mileage_text = block.find(string=re.compile(r"–ø—Ä–æ–±–µ–≥|–∫–º|—Ç—ã—Å", re.I))
        if mileage_text:
            parent_text = mileage_text.find_parent().get_text(strip=True)
            mileage_match = re.search(
                r"(\d+[\s\.]?\d*)\s*(—Ç—ã—Å\.?|—Ç\.?|–∫–º)", parent_text, re.I
            )
            if mileage_match:
                try:
                    mileage_val = float(
                        mileage_match.group(1).replace(" ", "").replace(".", "")
                    )
                    if (
                        "—Ç—ã—Å" in mileage_match.group(2).lower()
                        or "—Ç" in mileage_match.group(2).lower()
                    ):
                        mileage = int(mileage_val * 1000)
                    else:
                        mileage = int(mileage_val)
                    if mileage < 1000 or mileage > 1000000:
                        mileage = None
                except (ValueError, TypeError, OverflowError) as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–±–µ–≥–∞: {e}")
                    mileage = None

        region = ""
        region_tag = block.find(
            string=re.compile(
                r"–ú–æ—Å–∫–≤–∞|–°–ü–±|–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥|–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫|–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥|–ö–∞–∑–∞–Ω—å|–ù–∏–∂–Ω–∏–π|–ß–µ–ª—è–±–∏–Ω—Å–∫|"
                r"–û–º—Å–∫|–°–∞–º–∞—Ä–∞|–†–æ—Å—Ç–æ–≤|–£—Ñ–∞|–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫|–í–æ—Ä–æ–Ω–µ–∂|–ü–µ—Ä–º—å|–í–æ–ª–≥–æ–≥—Ä–∞–¥|–ù–∞–∑—Ä–∞–Ω—å|–ú–∞–≥–∞—Å|"
                r"–ò–Ω–≥—É—à–µ—Ç–∏—è|–ß–µ—á–Ω—è|–î–∞–≥–µ—Å—Ç–∞–Ω|–ì—Ä–æ–∑–Ω—ã–π|–î–µ—Ä–±–µ–Ω—Ç|–ú–∞—Ö–∞—á–∫–∞–ª–∞|–í–ª–∞–¥–∏–∫–∞–≤–∫–∞–∑",
                re.I,
            )
        )
        if region_tag:
            region = region_tag.find_parent().get_text(strip=True)[:50]

        img_tag = block.find("img", src=True)
        if img_tag:
            src = img_tag["src"].strip()
            photo_url = urljoin(BASE_URL, src) if src else None
        else:
            photo_url = None

        return {
            "source": "berkat.ru",
            "external_id": external_id,
            "title": title,
            "price": price,
            "brand": brand,
            "model": model[:100],
            "year": year,
            "mileage": mileage,
            "region": region,
            "url": url,
            "photo_url": photo_url,
            "parsed_at": datetime.now(timezone.utc).replace(tzinfo=None),
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–ª–æ–∫–∞: {e}", exc_info=True)
        return None


async def parse_berkat_pages(
    session: aiohttp.ClientSession, max_pages: int = 5
) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü berkat.ru."""
    all_ads = []
    logger.info(f"–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü berkat.ru")

    for page in range(1, max_pages + 1):
        url = f"{SEARCH_URL}?page={page}" if page > 1 else SEARCH_URL
        html = await fetch(session, url)
        if not html:
            logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            continue

        soup = BeautifulSoup(html, "lxml")
        ad_blocks = soup.select("div.board_list_item")

        if not ad_blocks:
            logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            continue

        page_ads = []
        for block in ad_blocks:
            ad_data = parse_ad_block(block)
            if ad_data:
                page_ads.append(ad_data)

        logger.info(
            f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(ad_blocks)} –±–ª–æ–∫–æ–≤, "
            f"—Å–ø–∞—Ä—Å–µ–Ω–æ {len(page_ads)} –∞–≤—Ç–æ-–æ–±—ä—è–≤–ª–µ–Ω–∏–π"
        )
        all_ads.extend(page_ads)

    logger.info(f"–í—Å–µ–≥–æ —Å–ø–∞—Ä—Å–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(all_ads)}")
    return all_ads


async def process_ads(ads: List[Dict]) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –Ω–∞–¥—ë–∂–Ω–æ–π –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é."""
    async with async_session() as db:
        active_filters = await get_all_active_filters(db)
        if not active_filters:
            logger.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
            return

        new_ads_count = 0
        notifications_sent = 0

        for ad_data in ads:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                existing = await get_ad_by_source_external(
                    db, ad_data["source"], ad_data["external_id"]
                )

                if existing:
                    ad = existing
                    # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                    ad.parsed_at = datetime.now(timezone.utc).replace(tzinfo=None)
                else:
                    ad = Ad(**ad_data)
                    db.add(ad)
                    await db.flush()  # –ü–æ–ª—É—á–∞–µ–º ID –¥–æ –∫–æ–º–º–∏—Ç–∞
                    new_ads_count += 1

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
                for filter_set in active_filters:
                    if not matches_filter(ad_data, filter_set):
                        continue

                    # üîí –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–ê: —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∑–∞–ø–∏—Å—å –≤ SentNotification?
                    stmt = (
                        await db.execute(
                            f"SELECT 1 FROM sent_notifications WHERE user_id = {filter_set.user_id} AND ad_id = {ad.id} AND filter_id = {filter_set.id}"
                        )
                    )
                    if stmt.scalar():
                        logger.debug(
                            f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ (—É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–æ—Å—å): "
                            f"–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å={filter_set.user_id}, –æ–±—ä—è–≤–ª–µ–Ω–∏–µ={ad.id}, —Ñ–∏–ª—å—Ç—Ä={filter_set.id}"
                        )
                        continue

                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
                    try:
                        await send_ad_notification(
                            telegram_id=filter_set.user_id,
                            ad=ad,
                            filter_name=filter_set.name,
                        )
                        
                        # üîí –°–û–ó–î–ê–Å–ú –ó–ê–ü–ò–°–¨ –í –ë–î –í –¢–û–ô –ñ–ï –¢–†–ê–ù–ó–ê–ö–¶–ò–ò
                        notification = SentNotification(
                            user_id=filter_set.user_id,
                            ad_id=ad.id,
                            filter_id=filter_set.id,
                        )
                        db.add(notification)
                        notifications_sent += 1
                        logger.info(
                            f"‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å={filter_set.user_id}, "
                            f"–æ–±—ä—è–≤–ª–µ–Ω–∏–µ={ad.id}, —Ñ–∏–ª—å—Ç—Ä={filter_set.id}"
                        )
                    except Exception as e:
                        logger.error(
                            f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {filter_set.user_id}: {e}"
                        )

            except Exception as e:
                logger.error(
                    f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è '{ad_data.get('title', 'N/A')}': {e}"
                )
                continue

        # üîí –ï–î–ò–ù–´–ô COMMIT ‚Äî –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤–º–µ—Å—Ç–µ
        await db.commit()
        logger.info(
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –Ω–æ–≤—ã—Ö: {new_ads_count}, "
            f"—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {notifications_sent}"
        )


async def berkat_parse_task_async() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ berkat.ru."""
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info(f"[{start_time}] –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ berkat.ru...")

    try:
        async with aiohttp.ClientSession() as session:
            ads = await parse_berkat_pages(session, max_pages=5)
            if ads:
                await process_ads(ads)
            else:
                logger.warning("–ê–≤—Ç–æ-–æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        logger.info(f"[{end_time}] –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    logger.info("–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ berkat.ru")
    asyncio.run(berkat_parse_task_async())